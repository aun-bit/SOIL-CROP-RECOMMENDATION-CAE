# Soil–Crop Recommendation System using Convolutional Autoencoder (CAE)

## Abstract / Overview

This project builds a **soil–crop recommendation system** that predicts the most suitable crop based on soil nutrients and environmental conditions. We use a **Convolutional Autoencoder (CAE)–based classifier** as the main model and a **Logistic Regression** baseline for comparison. Models are trained on the Kaggle **Crop Recommendation Dataset** and evaluated using accuracy, precision, recall, and F1-score. The work supports **SDG 2 (Zero Hunger)** and **SDG 12 (Responsible Consumption and Production)** by enabling data‑driven, sustainable crop planning.

---

## 1. Introduction

Crop choice is often based on experience or generic advisories, which may not fully reflect **field‑specific soil and climate conditions**. This can lead to suboptimal yields, wasted fertilizers and water, and economic losses. With soil testing and meteorological data becoming widely available, there is an opportunity to use **machine learning** for objective crop recommendation.

**Objectives:**

1. Design a **CAE‑based deep learning model** for soil–crop recommendation.
2. Implement a **Logistic Regression baseline** on standardized raw features.
3. Evaluate both models using accuracy, precision, recall, and F1-score.
4. Provide a simple **inference pipeline** to recommend crops for new soil inputs.
5. Relate the solution to **SDG 2** and **SDG 12**.

---

## 2. Dataset

- **Source:** Kaggle – Crop Recommendation Dataset (`Crop_recommendation.csv`).
- **Path:** `data/soil_crop/Crop_recommendation.csv`.
- **Size:** ~2200 samples, **22 crop classes**.
- **Features (7):**  
  `N, P, K, temperature, humidity, ph, rainfall`
- **Label:** `label` – crop name (22 classes).

**Preprocessing (implemented in `utils/data_loader.py`):**

- Stratified **train/validation/test split** (~70%/15%/15%).
- **Standardization** of features using `StandardScaler`.
- **Label encoding** using `LabelEncoder`.
- For CAE, features reshaped from `(7,)` to `(7, 1)` for Conv1D input.

---

## 3. Methodology

### 3.1 Baseline Model – Logistic Regression

- Input: standardized `[N, P, K, temperature, humidity, ph, rainfall]`.
- Model: scikit‑learn `LogisticRegression` (`max_iter=500`, `solver="lbfgs"`).
- Trained on train+val, evaluated on test.
- Script: `baseline_logreg.py`.

### 3.2 Main Model – CAE‑Based Classifier

**Encoder (Conv1D):**

- Input shape `(7, 1)`.
- `Conv1D(16, k=3, activation="relu", padding="same")`
- `Conv1D(8, k=3, strides=2, activation="relu", padding="same")`
- `Flatten()` → latent vector.

**Decoder (Autoencoder):**

- `UpSampling1D(2)`
- `Conv1D(16, k=3, activation="relu", padding="same")`
- `Conv1D(1, k=3, activation="linear", padding="same")`
- Crop to length 7 → reconstructed `(7, 1)`.

**Classifier head:**

- `Dense(64, "relu")`
- `Dense(22, "softmax")` (22 crops).

**Training:**

1. **Stage 1 – Autoencoder pretraining**  
   - Loss: MSE, Epochs: 20, Batch size: 32.
2. **Stage 2 – Classifier training**  
   - Loss: sparse categorical cross‑entropy, Epochs: 50, Batch size: 32, Optimizer: Adam.

Scripts: `model.py`, `train.py`, `evaluate.py`.

---

## 4. Experimental Setup

- **Splits:** 1540 train, 330 val, 330 test (stratified).
- **Metrics:** Accuracy, Precision (weighted), Recall (weighted), F1‑score (weighted).
- **Environment:**  
  - Python 3.11, TensorFlow/Keras, scikit‑learn, NumPy, Pandas.  
  - Developed in VS Code on Windows, also tested in Google Colab (CPU).

---

## 5. Results

### 5.1 Baseline vs Main Model

| Model                          | Accuracy | Precision (w) | Recall (w) | F1 (w)  |
|--------------------------------|----------|---------------|------------|---------|
| Logistic Regression (baseline) | 97.58%   | 97.64%        | 97.58%     | 97.55%  |
| CAE‑based classifier (main)    | 99.09%   | 99.15%        | 99.09%     | 99.09%  |

The Logistic Regression baseline is already very strong, but the CAE‑based classifier improves accuracy and F1 by about **+1.5 percentage points**, showing the benefit of non‑linear representation learning.

### 5.2 Visualizations

Images are in `docs/figures/` (generated by `visualize_results.py`):

- `ae_loss_curve.png` – Autoencoder training/validation loss vs epochs.  
- `classifier_loss_curve.png` – Classifier loss (train/val).  
- `classifier_accuracy_curve.png` – Classifier accuracy (train/val).  
- `confusion_matrix.png` – Confusion matrix on test set (nearly all mass on diagonal).  
- `roc_curve_micro.png`, `precision_recall_curve_micro.png` – Micro‑averaged ROC and PR curves showing high AUC/AP.

---

